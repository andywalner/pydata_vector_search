{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi + Lance Demo: Intelligent Recruitment Platform\n",
    "**(Hybrid Search + Analytics on the Lakehouse)**\n",
    "\n",
    "One table. Three query patterns. Zero data copying.\n",
    "\n",
    "1. Load real job postings from HuggingFace\n",
    "2. Ingest into a Hudi table with Lance vector embeddings\n",
    "3. **Vector Search** â€” match a resume by meaning\n",
    "4. **Hybrid Search** â€” add business constraints (SQL + vectors)\n",
    "5. **Analytics** â€” executive dashboard on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datasets import load_dataset\n",
    "import shutil, os, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TABLE_PATH = \"/tmp/hudi_recruiting_lake\"\n",
    "TABLE_NAME = \"job_market\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Spark/Lance emit log lines with surrogate characters that break\n",
    "# Jupyter's JSON serializer. Wrap both streams to sanitize them.\n",
    "class _SafeStream:\n",
    "    def __init__(self, stream):\n",
    "        self._stream = stream\n",
    "    def write(self, s):\n",
    "        return self._stream.write(s.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\"))\n",
    "    def flush(self):\n",
    "        return self._stream.flush()\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._stream, name)\n",
    "\n",
    "sys.stdout = _SafeStream(sys.stdout)\n",
    "sys.stderr = _SafeStream(sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start Spark with Hudi + Lance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/12 19:03:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark 3.5.3 ready with Hudi extensions.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"Recruiting-Lakehouse\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"\\u2713 Spark {spark.version} ready with Hudi extensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Job Postings from HuggingFace\n",
    "\n",
    "~3k data science job descriptions from [nathansutton/data-science-job-descriptions](https://huggingface.co/datasets/nathansutton/data-science-job-descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ds = load_dataset(\"nathansutton/data-science-job-descriptions\", split=\"train\")\n\njobs_data = []\nfor i, row in enumerate(ds):\n    jobs_data.append({\n        \"job_id\": f\"job_{i:04d}\",\n        \"company\": row[\"company\"],\n        \"title\": row[\"title\"],\n        \"job_description\": row[\"job_description\"],\n        \"text_for_vector\": f\"{row['title']} {row['job_description']}\"\n    })\n\ncompanies = set(r[\"company\"] for r in jobs_data)\nprint(f\"\\u2713 Loaded {len(jobs_data)} job postings from {len(companies)} companies.\\n\")\n\npreview = pd.DataFrame(jobs_data, columns=[\"job_id\", \"company\", \"title\", \"job_description\"])\npreview[\"job_description\"] = preview[\"job_description\"].str[:80] + \"...\"\npreview.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed & Ingest into the Lakehouse\n",
    "\n",
    "We embed every job description into a 384-dim vector, then write structured fields **and** embeddings into a single Hudi table using the Lance file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model = SentenceTransformer(EMBEDDING_MODEL)\nembeddings = model.encode([r[\"text_for_vector\"] for r in jobs_data], show_progress_bar=True)\n\nfor i, row in enumerate(jobs_data):\n    row[\"embedding\"] = embeddings[i].tolist()\n\nprint(f\"\\u2713 Generated {len(embeddings)} embeddings (dim={len(embeddings[0])}).\")\nprint(f\"  Each embedding is built from: title + job_description\\n\")\n\npreview = pd.DataFrame(jobs_data, columns=[\"job_id\", \"title\", \"job_description\", \"embedding\"])\npreview[\"job_description\"] = preview[\"job_description\"].str[:60] + \"...\"\npreview[\"embedding\"] = preview[\"embedding\"].apply(lambda v: str(v[:3])[:-1] + \", ...]\")\npreview.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n",
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[\u001b[0m2026-02-13T00:04:34Z \u001b[33mWARN \u001b[0m lance_file::v2::writer\u001b[90m]\u001b[0m You have requested an unstable format version.  Files written with this format version may not be readable in the future!  This is a development feature and should only be used for experimentation and never for production data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ingested 2921 jobs into Hudi table at /tmp/hudi_recruiting_lake\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"job_id\", StringType(), False),\n",
    "    StructField(\"company\", StringType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"job_description\", StringType(), False),\n",
    "    StructField(\"text_for_vector\", StringType(), False),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), False),\n",
    "])\n",
    "\n",
    "if os.path.exists(TABLE_PATH):\n",
    "    shutil.rmtree(TABLE_PATH)\n",
    "\n",
    "df = spark.createDataFrame(jobs_data, schema=schema)\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": TABLE_NAME,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"job_id\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"company\",\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.operation\": \"bulk_insert\",\n",
    "    \"hoodie.table.base.file.format\": \"lance\",\n",
    "    \"hoodie.write.record.merge.custom.implementation.classes\": \"org.apache.hudi.DefaultSparkRecordMerger\"\n",
    "}\n",
    "\n",
    "df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(TABLE_PATH)\n",
    "print(f\"\\u2713 Ingested {len(jobs_data)} jobs into Hudi table at {TABLE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Search â€” \"Upload\" a Resume\n",
    "\n",
    "A candidate uploads their resume. It never says *\"Senior Data Scientist\"* â€” it describes skills like *\"deployed LLMs\"* and *\"Scikit-Learn.\"* Can the system find the right jobs anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Resume Uploaded:\n",
      "EXPERIENCE:\n",
      "- 5 years building Machine Learning models using Python and Scikit-Learn.\n",
      "- Deployed Large Language Models (LLMs) to production.\n",
      "- Strong background in backend engineering and API design.\n"
     ]
    }
   ],
   "source": [
    "resume_text = \"\"\"\n",
    "EXPERIENCE:\n",
    "- 5 years building Machine Learning models using Python and Scikit-Learn.\n",
    "- Deployed Large Language Models (LLMs) to production.\n",
    "- Strong background in backend engineering and API design.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\U0001F4C4 Resume Uploaded:\\n{resume_text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Top Semantic Matches:\n",
      "  â€¢ Senior Data Scientist at United States Government â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Bestow â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Altoida â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Token Metrics â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Roc 360 â€” Score: 0.47\n"
     ]
    }
   ],
   "source": [
    "resume_vector = model.encode([resume_text])[0].tolist()\n",
    "spark.createDataFrame([(resume_vector,)], [\"q_vec\"]).createOrReplaceTempView(\"query_input\")\n",
    "\n",
    "matches = spark.sql(f\"\"\"\n",
    "    SELECT title, company, (1 - _distance) as score\n",
    "    FROM hudi_vector_search(\n",
    "        '{TABLE_PATH}', 'embedding', (SELECT q_vec FROM query_input), 5, 'cosine'\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"\\U0001F50E Top Semantic Matches:\")\n",
    "for row in matches:\n",
    "    print(f\"  \\u2022 {row.title} at {row.company} \\u2014 Score: {row.score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resume never mentions *\"Data Scientist\"* â€” but vector search finds them by **meaning**, not keywords.\n",
    "\n",
    "---\n",
    "## 5. Hybrid Search â€” Add Business Constraints\n",
    "\n",
    "The candidate says: *\"I specifically want to work at Reddit.\"*\n",
    "\n",
    "We combine the **same vector search** with a standard SQL `WHERE` clause. Vector + SQL in one query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Hybrid Matches (Reddit only):\n",
      "  â€¢ Senior Data Scientist at Reddit â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Reddit â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Reddit â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Reddit â€” Score: 0.47\n",
      "  â€¢ Senior Data Scientist at Reddit â€” Score: 0.47\n"
     ]
    }
   ],
   "source": [
    "# Post-filter approach: fetch a wide vector search window, then apply SQL filters.\n",
    "# hudi_vector_search() operates on the full vector index and doesn't yet support\n",
    "# predicate pushdown â€” so we retrieve broadly and filter after.\n",
    "# Future optimization: push filters directly into the vector index scan.\n",
    "\n",
    "hybrid_matches = spark.sql(f\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT title, company, (1 - _distance) as score\n",
    "        FROM hudi_vector_search(\n",
    "            '{TABLE_PATH}', 'embedding', (SELECT q_vec FROM query_input), 3000, 'cosine'\n",
    "        )\n",
    "    )\n",
    "    WHERE company = 'Reddit'\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 5\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"\\U0001F50E Hybrid Matches (Reddit only):\")\n",
    "for row in hybrid_matches:\n",
    "    print(f\"  \\u2022 {row.title} at {row.company} \\u2014 Score: {row.score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Hudi table. Same vector index. Just added a SQL filter.\n",
    "\n",
    "---\n",
    "## 6. Analytics Dashboard\n",
    "\n",
    "Now we switch hats â€” we're an analyst on the job platform team. Which companies are hiring the most? What roles dominate the market? We query the **exact same table**. No ETL to a separate warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"hudi\").load(TABLE_PATH).createOrReplaceTempView(\"jobs_table\")\n",
    "\n",
    "company_df = spark.sql(\"\"\"\n",
    "    SELECT company, count(*) as job_count\n",
    "    FROM jobs_table GROUP BY company\n",
    "    ORDER BY job_count DESC LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "title_df = spark.sql(\"\"\"\n",
    "    SELECT title, count(*) as title_count\n",
    "    FROM jobs_table GROUP BY title\n",
    "    ORDER BY title_count DESC LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].barh(company_df[\"company\"], company_df[\"job_count\"], color=\"green\")\n",
    "axes[0].set_title(\"Hiring Activity: Postings by Company\")\n",
    "axes[0].set_xlabel(\"Number of Postings\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(title_df[\"title\"], title_df[\"title_count\"], color=\"skyblue\")\n",
    "axes[1].set_title(\"Most Common Data Science Roles\")\n",
    "axes[1].set_xlabel(\"Number of Postings\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\u2713 Dashboard generated from the same Hudi table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**One table. Vector search, hybrid search, and analytics. No data copying, no separate vector database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}