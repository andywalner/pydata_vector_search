{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi + Lance Demo: Intelligent Recruitment Platform\n",
    "**(Hybrid Search + Analytics on the Lakehouse)**\n",
    "\n",
    "One table. Three query patterns. Zero data copying.\n",
    "\n",
    "1. Load real job postings from HuggingFace\n",
    "2. Ingest into a Hudi table with Lance vector embeddings\n",
    "3. **Vector Search** — match a resume by meaning\n",
    "4. **Hybrid Search** — add business constraints (SQL + vectors)\n",
    "5. **Analytics** — executive dashboard on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom datasets import load_dataset\nimport shutil, os, sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer\n\nTABLE_PATH = \"/tmp/hudi_recruiting_lake\"\nTABLE_NAME = \"job_market\"\nEMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n\n# Spark/Lance emit log lines with surrogate characters that break\n# Jupyter's JSON serializer. Wrap both streams to sanitize them.\nclass _SafeStream:\n    def __init__(self, stream):\n        self._stream = stream\n    def write(self, s):\n        return self._stream.write(s.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\"))\n    def flush(self):\n        return self._stream.flush()\n    def __getattr__(self, name):\n        return getattr(self._stream, name)\n\nsys.stdout = _SafeStream(sys.stdout)\nsys.stderr = _SafeStream(sys.stderr)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start Spark with Hudi + Lance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "spark = (SparkSession.builder.appName(\"Recruiting-Lakehouse\")\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n    .config(\"spark.ui.showConsoleProgress\", \"false\")\n    .getOrCreate())\n\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nprint(f\"\\u2713 Spark {spark.version} ready with Hudi extensions.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Job Postings from HuggingFace\n",
    "\n",
    "~3k data science job descriptions from [nathansutton/data-science-job-descriptions](https://huggingface.co/datasets/nathansutton/data-science-job-descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ds = load_dataset(\"nathansutton/data-science-job-descriptions\", split=\"train\")\n\njobs_data = []\nfor i, row in enumerate(ds):\n    jobs_data.append({\n        \"job_id\": f\"job_{i:04d}\",\n        \"company\": row[\"company\"],\n        \"title\": row[\"title\"],\n        \"job_description\": row[\"job_description\"],\n        \"text_for_vector\": f\"{row['title']} {row['job_description']}\"\n    })\n\ncompanies = set(r[\"company\"] for r in jobs_data)\nprint(f\"\\u2713 Loaded {len(jobs_data)} job postings from {len(companies)} companies.\\n\")\npd.DataFrame(jobs_data, columns=[\"job_id\", \"company\", \"title\"]).head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed & Ingest into the Lakehouse\n",
    "\n",
    "We embed every job description into a 384-dim vector, then write structured fields **and** embeddings into a single Hudi table using the Lance file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model = SentenceTransformer(EMBEDDING_MODEL)\nembeddings = model.encode([r[\"text_for_vector\"] for r in jobs_data], show_progress_bar=True)\n\nfor i, row in enumerate(jobs_data):\n    row[\"embedding\"] = embeddings[i].tolist()\n\nprint(f\"\\u2713 Generated {len(embeddings)} embeddings (dim={len(embeddings[0])}).\\n\")\n\npreview = pd.DataFrame(jobs_data, columns=[\"job_id\", \"title\", \"embedding\"])\npreview[\"embedding\"] = preview[\"embedding\"].apply(lambda v: str(v[:4])[:-1] + \", ...]\")\npreview.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "schema = StructType([\n    StructField(\"job_id\", StringType(), False),\n    StructField(\"company\", StringType(), False),\n    StructField(\"title\", StringType(), False),\n    StructField(\"job_description\", StringType(), False),\n    StructField(\"text_for_vector\", StringType(), False),\n    StructField(\"embedding\", ArrayType(FloatType()), False),\n])\n\nif os.path.exists(TABLE_PATH):\n    shutil.rmtree(TABLE_PATH)\n\ndf = spark.createDataFrame(jobs_data, schema=schema)\n\nhudi_options = {\n    \"hoodie.table.name\": TABLE_NAME,\n    \"hoodie.datasource.write.recordkey.field\": \"job_id\",\n    \"hoodie.datasource.write.partitionpath.field\": \"company\",\n    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n    \"hoodie.datasource.write.operation\": \"bulk_insert\",\n    \"hoodie.table.base.file.format\": \"lance\",\n    \"hoodie.write.record.merge.custom.implementation.classes\": \"org.apache.hudi.DefaultSparkRecordMerger\"\n}\n\ndf.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(TABLE_PATH)\nprint(f\"\\u2713 Ingested {len(jobs_data)} jobs into Hudi table at {TABLE_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Search — \"Upload\" a Resume\n",
    "\n",
    "A candidate uploads their resume. It never says *\"Senior Data Scientist\"* — it describes skills like *\"deployed LLMs\"* and *\"Scikit-Learn.\"* Can the system find the right jobs anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "resume_text = \"\"\"\nEXPERIENCE:\n- 5 years building Machine Learning models using Python and Scikit-Learn.\n- Deployed Large Language Models (LLMs) to production.\n- Strong background in backend engineering and API design.\n\"\"\"\n\nprint(f\"\\U0001F4C4 Resume Uploaded:\\n{resume_text.strip()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "resume_vector = model.encode([resume_text])[0].tolist()\nspark.createDataFrame([(resume_vector,)], [\"q_vec\"]).createOrReplaceTempView(\"query_input\")\n\nmatches = spark.sql(f\"\"\"\n    SELECT title, company, (1 - _distance) as score\n    FROM hudi_vector_search(\n        '{TABLE_PATH}', 'embedding', (SELECT q_vec FROM query_input), 5, 'cosine'\n    )\n\"\"\").collect()\n\nprint(\"\\U0001F50E Top Semantic Matches:\")\nfor row in matches:\n    print(f\"  \\u2022 {row.title} at {row.company} \\u2014 Score: {row.score:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resume never mentions *\"Data Scientist\"* — but vector search finds them by **meaning**, not keywords.\n",
    "\n",
    "---\n",
    "## 5. Hybrid Search — Add Business Constraints\n",
    "\n",
    "The candidate says: *\"I specifically want to work at Reddit.\"*\n",
    "\n",
    "We combine the **same vector search** with a standard SQL `WHERE` clause. Vector + SQL in one query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Post-filter approach: fetch a wide vector search window, then apply SQL filters.\n# hudi_vector_search() operates on the full vector index and doesn't yet support\n# predicate pushdown — so we retrieve broadly and filter after.\n# Future optimization: push filters directly into the vector index scan.\n\nhybrid_matches = spark.sql(f\"\"\"\n    SELECT * FROM (\n        SELECT title, company, (1 - _distance) as score\n        FROM hudi_vector_search(\n            '{TABLE_PATH}', 'embedding', (SELECT q_vec FROM query_input), 3000, 'cosine'\n        )\n    )\n    WHERE company = 'Reddit'\n    ORDER BY score DESC\n    LIMIT 5\n\"\"\").collect()\n\nprint(\"\\U0001F50E Hybrid Matches (Reddit only):\")\nfor row in hybrid_matches:\n    print(f\"  \\u2022 {row.title} at {row.company} \\u2014 Score: {row.score:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Hudi table. Same vector index. Just added a SQL filter.\n",
    "\n",
    "---\n",
    "## 6. Analytics Dashboard\n",
    "\n",
    "Now we switch hats — we're an analyst on the job platform team. Which companies are hiring the most? What roles dominate the market? We query the **exact same table**. No ETL to a separate warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"hudi\").load(TABLE_PATH).createOrReplaceTempView(\"jobs_table\")\n",
    "\n",
    "company_df = spark.sql(\"\"\"\n",
    "    SELECT company, count(*) as job_count\n",
    "    FROM jobs_table GROUP BY company\n",
    "    ORDER BY job_count DESC LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "title_df = spark.sql(\"\"\"\n",
    "    SELECT title, count(*) as title_count\n",
    "    FROM jobs_table GROUP BY title\n",
    "    ORDER BY title_count DESC LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].barh(company_df[\"company\"], company_df[\"job_count\"], color=\"green\")\n",
    "axes[0].set_title(\"Hiring Activity: Postings by Company\")\n",
    "axes[0].set_xlabel(\"Number of Postings\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(title_df[\"title\"], title_df[\"title_count\"], color=\"skyblue\")\n",
    "axes[1].set_title(\"Most Common Data Science Roles\")\n",
    "axes[1].set_xlabel(\"Number of Postings\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\u2713 Dashboard generated from the same Hudi table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**One table. Vector search, hybrid search, and analytics. No data copying, no separate vector database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}