{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi + Lance Demo: Intelligent Recruitment Platform\n",
    "**(Hybrid Search + Analytics on the Lakehouse)**\n",
    "\n",
    "### Flow:\n",
    "1. Load real job postings from HuggingFace\n",
    "2. User \"Uploads\" a Resume (Vector Search)\n",
    "3. Apply Business Rules (Hybrid Search: Vector + SQL Filters)\n",
    "4. Show Executive Dashboard (Analytics on the same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datasets import load_dataset\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"table_path\": \"/tmp/hudi_recruiting_lake\",\n",
    "    \"table_name\": \"job_market\",\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"clean_start\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    return (SparkSession.builder.appName(\"Recruiting-Lakehouse\")\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "            .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_job_data():\n    \"\"\"Load real data science job descriptions from HuggingFace.\"\"\"\n    print(\"Loading job descriptions from HuggingFace...\")\n    ds = load_dataset(\"nathansutton/data-science-job-descriptions\", split=\"train\")\n\n    data = []\n    for i, row in enumerate(ds):\n        data.append({\n            \"job_id\": f\"job_{i:04d}\",\n            \"company\": row[\"company\"],\n            \"title\": row[\"title\"],\n            \"job_description\": row[\"job_description\"],\n            \"text_for_vector\": f\"{row['title']} {row['job_description']}\"\n        })\n\n    companies = set(r[\"company\"] for r in data)\n    print(f\"\\u2713 Loaded {len(data)} job postings from {len(companies)} companies.\")\n    return data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingestion (The \"Lakehouse\" Foundation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ingest_data(spark, data):\n    # 1. Embed Descriptions\n    model = SentenceTransformer(CONFIG[\"embedding_model\"])\n    embeddings = model.encode([r[\"text_for_vector\"] for r in data], show_progress_bar=True)\n\n    for i, row in enumerate(data):\n        row[\"embedding\"] = embeddings[i].tolist()\n\n    # 2. Define Schema\n    schema = StructType([\n        StructField(\"job_id\", StringType(), False),\n        StructField(\"company\", StringType(), False),\n        StructField(\"title\", StringType(), False),\n        StructField(\"job_description\", StringType(), False),\n        StructField(\"text_for_vector\", StringType(), False),\n        StructField(\"embedding\", ArrayType(FloatType()), False),\n    ])\n\n    # 3. Write to Hudi (Lance Format)\n    if CONFIG[\"clean_start\"] and os.path.exists(CONFIG[\"table_path\"]):\n        shutil.rmtree(CONFIG[\"table_path\"])\n\n    df = spark.createDataFrame(data, schema=schema)\n\n    hudi_options = {\n        \"hoodie.table.name\": CONFIG[\"table_name\"],\n        \"hoodie.datasource.write.recordkey.field\": \"job_id\",\n        \"hoodie.datasource.write.partitionpath.field\": \"company\",\n        \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n        \"hoodie.datasource.write.operation\": \"upsert\",\n        \"hoodie.table.base.file.format\": \"lance\",\n        \"hoodie.write.record.merge.custom.implementation.classes\": \"org.apache.hudi.DefaultSparkRecordMerger\"\n    }\n\n    df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(CONFIG[\"table_path\"])\n    print(f\"\\u2713 Ingested {len(data)} jobs into the Lakehouse.\")\n    return model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Demo: Resume Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def demo_resume_matching(spark, model):\n    print(\"\\n\" + \"=\"*50)\n    print(\"DEMO PART 1: The 'Smart' Candidate Match\")\n    print(\"=\"*50)\n\n    # Simulate a Resume Upload\n    resume_text = \"\"\"\n    EXPERIENCE:\n    - 5 years building Machine Learning models using Python and Scikit-Learn.\n    - Deployed Large Language Models (LLMs) to production.\n    - Strong background in backend engineering and API design.\n    \"\"\"\n    print(f\"\\ud83d\\udcc4 User Resume Uploaded: \\n{resume_text.strip()}\\n\")\n\n    # Vectorize Resume\n    resume_vector = model.encode([resume_text])[0].tolist()\n\n    # Register Query Vector\n    spark.createDataFrame([(resume_vector,)], [\"q_vec\"]).createOrReplaceTempView(\"query_input\")\n\n    # --- SCENARIO A: Pure Vector Search ---\n    print(\"\\ud83d\\udd0e Executing Vector Search (Semantic Match)...\")\n    matches = spark.sql(f\"\"\"\n        SELECT title, company, (1 - _distance) as score\n        FROM hudi_vector_search(\n            '{CONFIG['table_path']}', 'embedding', (SELECT q_vec FROM query_input), 5, 'cosine'\n        )\n    \"\"\").collect()\n\n    print(\"\\nTop Matches for your Resume:\")\n    for row in matches:\n        print(f\"  \\u2022 {row.title} at {row.company} \\u2014 Score: {row.score:.2f}\")\n\n    # --- SCENARIO B: Hybrid Search (The Business Requirement) ---\n    print(\"\\n\\u26a0\\ufe0f  User Feedback: 'I specifically want to work at Reddit.'\")\n    print(\"\\ud83d\\udd0e Executing Hybrid Search (Vector + SQL Filters)...\")\n\n    hybrid_query = f\"\"\"\n        SELECT * FROM (\n            SELECT title, company, (1 - _distance) as score\n            FROM hudi_vector_search(\n                '{CONFIG['table_path']}', 'embedding', (SELECT q_vec FROM query_input), 50, 'cosine'\n            )\n        )\n        WHERE company = 'Reddit'\n        ORDER BY score DESC\n        LIMIT 5\n    \"\"\"\n    hybrid_matches = spark.sql(hybrid_query).collect()\n\n    print(\"\\nTop HYBRID Matches (Reddit only):\")\n    if not hybrid_matches:\n        print(\"  (No matches found with these constraints)\")\n    for row in hybrid_matches:\n        print(f\"  \\u2022 {row.title} at {row.company} \\u2014 Score: {row.score:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Demo: Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def demo_analytics_dashboard(spark):\n    print(\"\\n\" + \"=\"*50)\n    print(\"DEMO PART 2: The Executive Dashboard\")\n    print(\"Value: The SAME data matches resumes AND powers BI.\")\n    print(\"=\"*50)\n\n    spark.read.format(\"hudi\").load(CONFIG[\"table_path\"]).createOrReplaceTempView(\"jobs_table\")\n\n    # 1. Hiring Activity by Company\n    print(\"Generating 'Hiring Activity' Chart...\")\n    company_df = spark.sql(\"\"\"\n        SELECT company, count(*) as job_count\n        FROM jobs_table\n        GROUP BY company\n        ORDER BY job_count DESC\n        LIMIT 15\n    \"\"\").toPandas()\n\n    # 2. Most Common Job Titles\n    print(\"Generating 'Top Roles' Chart...\")\n    title_df = spark.sql(\"\"\"\n        SELECT title, count(*) as title_count\n        FROM jobs_table\n        GROUP BY title\n        ORDER BY title_count DESC\n        LIMIT 15\n    \"\"\").toPandas()\n\n    # PLOTTING\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    # Plot 1: Hiring Activity\n    axes[0].barh(company_df[\"company\"], company_df[\"job_count\"], color=\"green\")\n    axes[0].set_title(\"Hiring Activity: Postings by Company\")\n    axes[0].set_xlabel(\"Number of Postings\")\n    axes[0].invert_yaxis()\n\n    # Plot 2: Top Roles\n    axes[1].barh(title_df[\"title\"], title_df[\"title_count\"], color=\"skyblue\")\n    axes[1].set_title(\"Most Common Data Science Roles\")\n    axes[1].set_xlabel(\"Number of Postings\")\n    axes[1].invert_yaxis()\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\u2713 Dashboard generated from Hudi table.\")\n    print(\"  (In a real app, this would be a live Streamlit/Tableau view)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = load_job_data()\n",
    "model = ingest_data(spark, jobs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_resume_matching(spark, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_analytics_dashboard(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}