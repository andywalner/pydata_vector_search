{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi + Lance Demo: Intelligent Recruitment Platform\n",
    "**(Hybrid Search + Analytics on the Lakehouse)**\n",
    "\n",
    "### Flow:\n",
    "1. Ingest Job Postings (Structured + Unstructured Data)\n",
    "2. User \"Uploads\" a Resume (Vector Search)\n",
    "3. Apply Business Rules (Hybrid Search: Vector + SQL Filters)\n",
    "4. Show Executive Dashboard (Analytics on the same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime, count, avg, date_format\n",
    "from pyspark.sql.types import *\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"table_path\": \"/tmp/hudi_recruiting_lake\",\n",
    "    \"table_name\": \"job_market\",\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"clean_start\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    return (SparkSession.builder.appName(\"Recruiting-Lakehouse\")\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "            .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_jobs():\n",
    "    \"\"\"Generates rich data: Text for AI, Numbers for BI.\"\"\"\n",
    "    print(\"Generating 100 job postings with historical dates...\")\n",
    "    \n",
    "    titles = [\n",
    "        (\"Senior Data Scientist\", \"Build LLM applications and predictive models. Python, PyTorch.\", \"Tech\"),\n",
    "        (\"Marketing Director\", \"Lead global brand strategy and social media campaigns.\", \"Marketing\"),\n",
    "        (\"Solutions Architect\", \"Design cloud infrastructure on AWS and Azure for enterprise clients.\", \"Tech\"),\n",
    "        (\"HR Business Partner\", \"Manage employee relations and internal hiring strategy.\", \"HR\"),\n",
    "        (\"Frontend Developer\", \"React and TypeScript expert needed for high-traffic e-commerce site.\", \"Tech\"),\n",
    "        (\"Sales Executive\", \"B2B enterprise sales. High commission. Travel required.\", \"Sales\")\n",
    "    ]\n",
    "    \n",
    "    cities = [\"New York\", \"San Francisco\", \"Austin\", \"London\", \"Remote\"]\n",
    "    \n",
    "    data = []\n",
    "    base_date = datetime.now()\n",
    "    \n",
    "    for i in range(100):\n",
    "        t_info = random.choice(titles)\n",
    "        city = random.choice(cities)\n",
    "        # Salary varies by city/role logic (simplified)\n",
    "        base_sal = 160000 if t_info[2] == \"Tech\" else 90000\n",
    "        salary = int(base_sal * random.uniform(0.8, 1.2))\n",
    "        \n",
    "        # Date distribution (last 6 months)\n",
    "        post_date = base_date - timedelta(days=random.randint(0, 180))\n",
    "        \n",
    "        data.append({\n",
    "            \"job_id\": f\"job_{i:03d}\",\n",
    "            \"title\": t_info[0],\n",
    "            \"description\": t_info[1] + f\" Located in {city}.\",\n",
    "            \"department\": t_info[2],\n",
    "            \"location\": city,\n",
    "            \"salary\": salary,\n",
    "            \"posted_at\": int(post_date.timestamp()), # For time-series analytics\n",
    "            \"text_for_vector\": f\"{t_info[0]} {t_info[1]}\"\n",
    "        })\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingestion (The \"Lakehouse\" Foundation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(spark, data):\n",
    "    # 1. Embed Descriptions\n",
    "    model = SentenceTransformer(CONFIG['embedding_model'])\n",
    "    embeddings = model.encode([r['text_for_vector'] for r in data])\n",
    "    \n",
    "    for i, row in enumerate(data):\n",
    "        row['embedding'] = embeddings[i].tolist()\n",
    "\n",
    "    # 2. Define Schema\n",
    "    schema = StructType([\n",
    "        StructField(\"job_id\", StringType(), False),\n",
    "        StructField(\"title\", StringType(), False),\n",
    "        StructField(\"description\", StringType(), False),\n",
    "        StructField(\"department\", StringType(), False),\n",
    "        StructField(\"location\", StringType(), False),\n",
    "        StructField(\"salary\", IntegerType(), False),\n",
    "        StructField(\"posted_at\", LongType(), False), # Timestamp\n",
    "        StructField(\"text_for_vector\", StringType(), False),\n",
    "        StructField(\"embedding\", ArrayType(FloatType()), False),\n",
    "    ])\n",
    "\n",
    "    # 3. Write to Hudi (Lance Format)\n",
    "    if CONFIG[\"clean_start\"] and os.path.exists(CONFIG[\"table_path\"]):\n",
    "        shutil.rmtree(CONFIG[\"table_path\"])\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "    \n",
    "    hudi_options = {\n",
    "        \"hoodie.table.name\": CONFIG[\"table_name\"],\n",
    "        \"hoodie.datasource.write.recordkey.field\": \"job_id\",\n",
    "        \"hoodie.datasource.write.partitionpath.field\": \"department\",\n",
    "        \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "        \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "        \"hoodie.table.base.file.format\": \"lance\",\n",
    "        \"hoodie.write.record.merge.custom.implementation.classes\": \"org.apache.hudi.DefaultSparkRecordMerger\"\n",
    "    }\n",
    "\n",
    "    df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(CONFIG[\"table_path\"])\n",
    "    print(f\"\\u2713 Ingested {len(data)} jobs into the Lakehouse.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Demo: Resume Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_resume_matching(spark, model):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO PART 1: The 'Smart' Candidate Match\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Simulate a Resume Upload\n",
    "    resume_text = \"\"\"\n",
    "    EXPERIENCE:\n",
    "    - 5 years building Machine Learning models using Python and Scikit-Learn.\n",
    "    - Deployed Large Language Models (LLMs) to production.\n",
    "    - Strong background in backend engineering and API design.\n",
    "    \"\"\"\n",
    "    print(f\"\\ud83d\\udcc4 User Resume Uploaded: \\n{resume_text.strip()}\\n\")\n",
    "    \n",
    "    # Vectorize Resume\n",
    "    resume_vector = model.encode([resume_text])[0].tolist()\n",
    "    \n",
    "    # Register Query Vector\n",
    "    spark.createDataFrame([(resume_vector,)], [\"q_vec\"]).createOrReplaceTempView(\"query_input\")\n",
    "    \n",
    "    # --- SCENARIO A: Pure Vector Search ---\n",
    "    print(\"\\ud83d\\udd0e Executing Vector Search (Semantic Match)...\")\n",
    "    matches = spark.sql(f\"\"\"\n",
    "        SELECT title, location, salary, (1 - _distance) as score\n",
    "        FROM hudi_vector_search(\n",
    "            '{CONFIG['table_path']}', 'embedding', (SELECT q_vec FROM query_input), 5, 'cosine'\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    print(\"\\nTop Matches for your Resume:\")\n",
    "    for row in matches:\n",
    "        print(f\"  \\u2022 {row.title} ({row.location}) - Match Score: {row.score:.2f}\")\n",
    "\n",
    "    # --- SCENARIO B: Hybrid Search (The Business Requirement) ---\n",
    "    print(\"\\n\\u26a0\\ufe0f  User Feedback: 'I only want Remote jobs paying > $150k'\")\n",
    "    print(\"\\ud83d\\udd0e Executing Hybrid Search (Vector + SQL Filters)...\")\n",
    "    \n",
    "    hybrid_query = f\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT title, location, salary, (1 - _distance) as score\n",
    "            FROM hudi_vector_search(\n",
    "                '{CONFIG['table_path']}', 'embedding', (SELECT q_vec FROM query_input), 20, 'cosine'\n",
    "            )\n",
    "        ) \n",
    "        WHERE location = 'Remote' AND salary > 150000\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 5\n",
    "    \"\"\"\n",
    "    hybrid_matches = spark.sql(hybrid_query).collect()\n",
    "    \n",
    "    print(\"\\nTop HYBRID Matches:\")\n",
    "    if not hybrid_matches:\n",
    "        print(\"  (No matches found with these strict constraints - this is also a valuable insight!)\")\n",
    "    for row in hybrid_matches:\n",
    "        print(f\"  \\u2022 {row.title} [${row.salary:,}] - {row.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Demo: Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_analytics_dashboard(spark):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO PART 2: The Executive Dashboard\")\n",
    "    print(\"Value: The SAME data matches resumes AND powers BI.\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    spark.read.format(\"hudi\").load(CONFIG[\"table_path\"]).createOrReplaceTempView(\"jobs_table\")\n",
    "    \n",
    "    # 1. Time Series Data (Job Postings per Month)\n",
    "    print(\"Generating 'Job Trends' Chart...\")\n",
    "    trend_df = spark.sql(\"\"\"\n",
    "        SELECT from_unixtime(posted_at, 'yyyy-MM') as month, count(*) as count\n",
    "        FROM jobs_table\n",
    "        GROUP BY month ORDER BY month\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    # 2. Salary Analysis by Dept\n",
    "    print(\"Generating 'Salary Insights' Chart...\")\n",
    "    salary_df = spark.sql(\"\"\"\n",
    "        SELECT department, avg(salary) as avg_salary\n",
    "        FROM jobs_table GROUP BY department\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    # PLOTTING\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Trend\n",
    "    axes[0].plot(trend_df['month'], trend_df['count'], marker='o', color='green')\n",
    "    axes[0].set_title(\"Market Demand: Job Postings Over Time\")\n",
    "    axes[0].set_xlabel(\"Month\")\n",
    "    axes[0].set_ylabel(\"New Jobs\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Salary\n",
    "    axes[1].bar(salary_df['department'], salary_df['avg_salary'], color='skyblue')\n",
    "    axes[1].set_title(\"Compensation Benchmark by Dept\")\n",
    "    axes[1].set_ylabel(\"Avg Salary ($)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\u2713 Dashboard generated from Hudi table.\")\n",
    "    print(\"  (In a real app, this would be a live Streamlit/Tableau view)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = generate_realistic_jobs()\n",
    "model = ingest_data(spark, jobs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_resume_matching(spark, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_analytics_dashboard(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}