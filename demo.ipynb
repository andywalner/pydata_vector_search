{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi + Lance Demo: Intelligent Recruitment Platform\n",
    "**(Hybrid Search + Analytics on the Lakehouse)**\n",
    "\n",
    "### Flow:\n",
    "1. Load real job postings from HuggingFace\n",
    "2. User \"Uploads\" a Resume (Vector Search)\n",
    "3. Apply Business Rules (Hybrid Search: Vector + SQL Filters)\n",
    "4. Show Executive Dashboard (Analytics on the same data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datasets import load_dataset\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"table_path\": \"/tmp/hudi_recruiting_lake\",\n",
    "    \"table_name\": \"job_market\",\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"clean_start\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    return (SparkSession.builder.appName(\"Recruiting-Lakehouse\")\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "            .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_job_data():\n",
    "    \"\"\"Load real job descriptions from HuggingFace.\"\"\"\n",
    "    print(\"Loading job descriptions from HuggingFace...\")\n",
    "    ds = load_dataset(\"jacob-hugging-face/job-descriptions\", split=\"train\")\n",
    "\n",
    "    data = []\n",
    "    for i, row in enumerate(ds):\n",
    "        data.append({\n",
    "            \"job_id\": f\"job_{i:03d}\",\n",
    "            \"company_name\": row[\"company_name\"],\n",
    "            \"position_title\": row[\"position_title\"],\n",
    "            \"job_description\": row[\"job_description\"],\n",
    "            \"description_length\": row[\"description_length\"],\n",
    "            \"text_for_vector\": f\"{row['position_title']} {row['job_description']}\"\n",
    "        })\n",
    "\n",
    "    companies = set(r[\"company_name\"] for r in data)\n",
    "    print(f\"\\u2713 Loaded {len(data)} job postings from {len(companies)} companies.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingestion (The \"Lakehouse\" Foundation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(spark, data):\n",
    "    # 1. Embed Descriptions\n",
    "    model = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
    "    embeddings = model.encode([r[\"text_for_vector\"] for r in data], show_progress_bar=True)\n",
    "\n",
    "    for i, row in enumerate(data):\n",
    "        row[\"embedding\"] = embeddings[i].tolist()\n",
    "\n",
    "    # 2. Define Schema\n",
    "    schema = StructType([\n",
    "        StructField(\"job_id\", StringType(), False),\n",
    "        StructField(\"company_name\", StringType(), False),\n",
    "        StructField(\"position_title\", StringType(), False),\n",
    "        StructField(\"job_description\", StringType(), False),\n",
    "        StructField(\"description_length\", IntegerType(), False),\n",
    "        StructField(\"text_for_vector\", StringType(), False),\n",
    "        StructField(\"embedding\", ArrayType(FloatType()), False),\n",
    "    ])\n",
    "\n",
    "    # 3. Write to Hudi (Lance Format)\n",
    "    if CONFIG[\"clean_start\"] and os.path.exists(CONFIG[\"table_path\"]):\n",
    "        shutil.rmtree(CONFIG[\"table_path\"])\n",
    "\n",
    "    df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "    hudi_options = {\n",
    "        \"hoodie.table.name\": CONFIG[\"table_name\"],\n",
    "        \"hoodie.datasource.write.recordkey.field\": \"job_id\",\n",
    "        \"hoodie.datasource.write.partitionpath.field\": \"company_name\",\n",
    "        \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "        \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "        \"hoodie.table.base.file.format\": \"lance\",\n",
    "        \"hoodie.write.record.merge.custom.implementation.classes\": \"org.apache.hudi.DefaultSparkRecordMerger\"\n",
    "    }\n",
    "\n",
    "    df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(CONFIG[\"table_path\"])\n",
    "    print(f\"\\u2713 Ingested {len(data)} jobs into the Lakehouse.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Demo: Resume Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_resume_matching(spark, model):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO PART 1: The 'Smart' Candidate Match\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Simulate a Resume Upload\n",
    "    resume_text = \"\"\"\n",
    "    EXPERIENCE:\n",
    "    - 5 years building Machine Learning models using Python and Scikit-Learn.\n",
    "    - Deployed Large Language Models (LLMs) to production.\n",
    "    - Strong background in backend engineering and API design.\n",
    "    \"\"\"\n",
    "    print(f\"\\ud83d\\udcc4 User Resume Uploaded: \\n{resume_text.strip()}\\n\")\n",
    "\n",
    "    # Vectorize Resume\n",
    "    resume_vector = model.encode([resume_text])[0].tolist()\n",
    "\n",
    "    # Register Query Vector\n",
    "    spark.createDataFrame([(resume_vector,)], [\"q_vec\"]).createOrReplaceTempView(\"query_input\")\n",
    "\n",
    "    # --- SCENARIO A: Pure Vector Search ---\n",
    "    print(\"\\ud83d\\udd0e Executing Vector Search (Semantic Match)...\")\n",
    "    matches = spark.sql(f\"\"\"\n",
    "        SELECT position_title, company_name, (1 - _distance) as score\n",
    "        FROM hudi_vector_search(\n",
    "            '{CONFIG['table_path']}', 'embedding', (SELECT q_vec FROM query_input), 5, 'cosine'\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "\n",
    "    print(\"\\nTop Matches for your Resume:\")\n",
    "    for row in matches:\n",
    "        print(f\"  \\u2022 {row.position_title} at {row.company_name} \\u2014 Score: {row.score:.2f}\")\n",
    "\n",
    "    # --- SCENARIO B: Hybrid Search (The Business Requirement) ---\n",
    "    print(\"\\n\\u26a0\\ufe0f  User Feedback: 'I specifically want to work at Google.'\")\n",
    "    print(\"\\ud83d\\udd0e Executing Hybrid Search (Vector + SQL Filters)...\")\n",
    "\n",
    "    hybrid_query = f\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT position_title, company_name, (1 - _distance) as score\n",
    "            FROM hudi_vector_search(\n",
    "                '{CONFIG['table_path']}', 'embedding', (SELECT q_vec FROM query_input), 50, 'cosine'\n",
    "            )\n",
    "        )\n",
    "        WHERE company_name = 'Google'\n",
    "        ORDER BY score DESC\n",
    "        LIMIT 5\n",
    "    \"\"\"\n",
    "    hybrid_matches = spark.sql(hybrid_query).collect()\n",
    "\n",
    "    print(\"\\nTop HYBRID Matches (Google only):\")\n",
    "    if not hybrid_matches:\n",
    "        print(\"  (No matches found with these constraints)\")\n",
    "    for row in hybrid_matches:\n",
    "        print(f\"  \\u2022 {row.position_title} at {row.company_name} \\u2014 Score: {row.score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Demo: Analytics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_analytics_dashboard(spark):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DEMO PART 2: The Executive Dashboard\")\n",
    "    print(\"Value: The SAME data matches resumes AND powers BI.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    spark.read.format(\"hudi\").load(CONFIG[\"table_path\"]).createOrReplaceTempView(\"jobs_table\")\n",
    "\n",
    "    # 1. Hiring Activity by Company\n",
    "    print(\"Generating 'Hiring Activity' Chart...\")\n",
    "    company_df = spark.sql(\"\"\"\n",
    "        SELECT company_name, count(*) as job_count\n",
    "        FROM jobs_table\n",
    "        GROUP BY company_name\n",
    "        ORDER BY job_count DESC\n",
    "        LIMIT 15\n",
    "    \"\"\").toPandas()\n",
    "\n",
    "    # 2. Avg Description Length by Company\n",
    "    print(\"Generating 'Description Detail' Chart...\")\n",
    "    detail_df = spark.sql(\"\"\"\n",
    "        SELECT company_name, avg(description_length) as avg_length\n",
    "        FROM jobs_table\n",
    "        GROUP BY company_name\n",
    "        ORDER BY avg_length DESC\n",
    "        LIMIT 15\n",
    "    \"\"\").toPandas()\n",
    "\n",
    "    # PLOTTING\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot 1: Hiring Activity\n",
    "    axes[0].barh(company_df[\"company_name\"], company_df[\"job_count\"], color=\"green\")\n",
    "    axes[0].set_title(\"Hiring Activity: Postings by Company\")\n",
    "    axes[0].set_xlabel(\"Number of Postings\")\n",
    "    axes[0].invert_yaxis()\n",
    "\n",
    "    # Plot 2: Description Detail\n",
    "    axes[1].barh(detail_df[\"company_name\"], detail_df[\"avg_length\"], color=\"skyblue\")\n",
    "    axes[1].set_title(\"Description Detail by Company\")\n",
    "    axes[1].set_xlabel(\"Avg Description Length (chars)\")\n",
    "    axes[1].invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\u2713 Dashboard generated from Hudi table.\")\n",
    "    print(\"  (In a real app, this would be a live Streamlit/Tableau view)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = load_job_data()\n",
    "model = ingest_data(spark, jobs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_resume_matching(spark, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_analytics_dashboard(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}