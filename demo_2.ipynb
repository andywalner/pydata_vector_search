{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi + Lance Demo: Intelligent Recruitment Platform\n",
    "**(Hybrid Search + Analytics on the Lakehouse)**\n",
    "\n",
    "One table. Three query patterns. Zero data copying.\n",
    "\n",
    "1. Load real job postings from HuggingFace\n",
    "2. Ingest into a Hudi table with Lance vector embeddings\n",
    "3. **Vector Search** — match a resume by meaning\n",
    "4. **Hybrid Search** — add business constraints (SQL + vectors)\n",
    "5. **Analytics** — executive dashboard on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from datasets import load_dataset\n",
    "import shutil, os, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "TABLE_PATH = \"/tmp/hudi_recruiting_lake_nopart\"\n",
    "TABLE_NAME = \"job_market_nopart\"\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Spark/Lance emit log lines with surrogate characters that break\n",
    "# Jupyter's JSON serializer. Wrap both streams to sanitize them.\n",
    "class _SafeStream:\n",
    "    def __init__(self, stream):\n",
    "        self._stream = stream\n",
    "    def write(self, s):\n",
    "        return self._stream.write(s.encode(\"utf-8\", errors=\"replace\").decode(\"utf-8\"))\n",
    "    def flush(self):\n",
    "        return self._stream.flush()\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._stream, name)\n",
    "\n",
    "sys.stdout = _SafeStream(sys.stdout)\n",
    "sys.stderr = _SafeStream(sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start Spark with Hudi + Lance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"Recruiting-Lakehouse\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"\\u2713 Spark {spark.version} ready with Hudi extensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Job Postings from HuggingFace\n",
    "\n",
    "~3k data science job descriptions from [nathansutton/data-science-job-descriptions](https://huggingface.co/datasets/nathansutton/data-science-job-descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"nathansutton/data-science-job-descriptions\", split=\"train\")\n",
    "\n",
    "jobs_data = []\n",
    "for i, row in enumerate(ds):\n",
    "    jobs_data.append({\n",
    "        \"job_id\": f\"job_{i:04d}\",\n",
    "        \"company\": row[\"company\"],\n",
    "        \"title\": row[\"title\"],\n",
    "        \"job_description\": row[\"job_description\"],\n",
    "        \"text_for_vector\": f\"{row['title']} {row['job_description']}\"\n",
    "    })\n",
    "\n",
    "companies = set(r[\"company\"] for r in jobs_data)\n",
    "print(f\"\\u2713 Loaded {len(jobs_data)} job postings from {len(companies)} companies.\\n\")\n",
    "\n",
    "preview = pd.DataFrame(jobs_data, columns=[\"job_id\", \"company\", \"title\", \"job_description\"])\n",
    "preview[\"job_description\"] = preview[\"job_description\"].str[:80] + \"...\"\n",
    "preview.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed & Ingest into the Lakehouse\n",
    "\n",
    "We embed every job description into a 384-dim vector, then write structured fields **and** embeddings into a single Hudi table using the Lance file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "embeddings = model.encode([r[\"text_for_vector\"] for r in jobs_data], show_progress_bar=True)\n",
    "\n",
    "for i, row in enumerate(jobs_data):\n",
    "    row[\"embedding\"] = embeddings[i].tolist()\n",
    "\n",
    "print(f\"\\u2713 Generated {len(embeddings)} embeddings (dim={len(embeddings[0])}).\")\n",
    "print(f\"  Each embedding is built from: title + job_description\\n\")\n",
    "\n",
    "preview = pd.DataFrame(jobs_data, columns=[\"job_id\", \"title\", \"job_description\", \"embedding\"])\n",
    "preview[\"job_description\"] = preview[\"job_description\"].str[:60] + \"...\"\n",
    "preview[\"embedding\"] = preview[\"embedding\"].apply(lambda v: str(v[:3])[:-1] + \", ...]\")\n",
    "preview.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "schema = StructType([\n    StructField(\"job_id\", StringType(), False),\n    StructField(\"company\", StringType(), False),\n    StructField(\"title\", StringType(), False),\n    StructField(\"job_description\", StringType(), False),\n    StructField(\"text_for_vector\", StringType(), False),\n    StructField(\"embedding\", ArrayType(FloatType()), False),\n])\n\nif os.path.exists(TABLE_PATH):\n    shutil.rmtree(TABLE_PATH)\n\ndf = spark.createDataFrame(jobs_data, schema=schema)\n\nhudi_options = {\n    \"hoodie.table.name\": TABLE_NAME,\n    \"hoodie.datasource.write.recordkey.field\": \"job_id\",\n    \"hoodie.datasource.write.partitionpath.field\": \"\",\n    \"hoodie.datasource.write.keygenerator.class\": \"org.apache.hudi.keygen.NonpartitionedKeyGenerator\",\n    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n    \"hoodie.datasource.write.operation\": \"upsert\",\n    \"hoodie.table.base.file.format\": \"lance\",\n    \"hoodie.write.record.merge.custom.implementation.classes\": \"org.apache.hudi.DefaultSparkRecordMerger\"\n}\n\ndf.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(TABLE_PATH)\nprint(f\"\\u2713 Ingested {len(jobs_data)} jobs into Hudi table at {TABLE_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Search — \"Upload\" a Resume\n",
    "\n",
    "A candidate uploads their resume. They have experience in **product growth, A/B testing, and experimentation** — but the resume never uses the exact job title *\"Data Scientist Growth.\"* Can the system find the right roles anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = \"\"\"\n",
    "EXPERIENCE:\n",
    "- 4 years driving product growth through data science and experimentation.\n",
    "- Built A/B testing frameworks and analyzed user funnels to optimize conversion.\n",
    "- Led causal inference studies measuring the impact of new product features.\n",
    "- Python, SQL, Bayesian statistics, and machine learning for growth modeling.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\U0001F4C4 Resume Uploaded:\\n{resume_text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_vector = model.encode([resume_text])[0].tolist()\n",
    "spark.createDataFrame([(resume_vector,)], [\"q_vec\"]).createOrReplaceTempView(\"query_input\")\n",
    "\n",
    "# Read the full table for joining (vector search can misalign non-vector columns)\n",
    "spark.read.format(\"hudi\").load(TABLE_PATH).createOrReplaceTempView(\"jobs_table\")\n",
    "\n",
    "matches_df = spark.sql(f\"\"\"\n",
    "    SELECT j.title, j.company, j.job_description, round(1 - v._distance, 2) as score\n",
    "    FROM hudi_vector_search(\n",
    "        '{TABLE_PATH}', 'embedding', (SELECT q_vec FROM query_input), 10, 'cosine'\n",
    "    ) v\n",
    "    JOIN jobs_table j ON v.job_id = j.job_id\n",
    "\"\"\").toPandas()\n",
    "\n",
    "matches_df[\"job_description\"] = matches_df[\"job_description\"].str.strip().str[:100] + \"...\"\n",
    "\n",
    "print(\"\\U0001F50E Top Semantic Matches:\\n\")\n",
    "matches_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resume never mentions *\"Data Scientist\"* or *\"Growth\"* as a job title — but vector search finds growth-focused roles by **meaning**, not keywords.\n",
    "\n",
    "---\n",
    "## 5. Hybrid Search — Add Business Constraints\n",
    "\n",
    "The candidate says: *\"I specifically want to work at Reddit.\"*\n",
    "\n",
    "We combine the **same vector search** with a standard SQL `WHERE` clause. Vector + SQL in one query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-filter approach: fetch a wide vector search window, then apply SQL filters.\n",
    "# hudi_vector_search() operates on the full vector index and doesn't yet support\n",
    "# predicate pushdown — so we retrieve broadly and filter after.\n",
    "# Future optimization: push filters directly into the vector index scan.\n",
    "\n",
    "hybrid_df = spark.sql(f\"\"\"\n",
    "    SELECT j.title, j.company, j.job_description, round(1 - v._distance, 2) as score\n",
    "    FROM hudi_vector_search(\n",
    "        '{TABLE_PATH}', 'embedding', (SELECT q_vec FROM query_input), 3000, 'cosine'\n",
    "    ) v\n",
    "    JOIN jobs_table j ON v.job_id = j.job_id\n",
    "    WHERE j.company = 'Reddit'\n",
    "    ORDER BY score DESC\n",
    "    LIMIT 5\n",
    "\"\"\").toPandas()\n",
    "\n",
    "hybrid_df[\"job_description\"] = hybrid_df[\"job_description\"].str.strip().str[:100] + \"...\"\n",
    "\n",
    "print(\"\\U0001F50E Hybrid Matches (Reddit only):\\n\")\n",
    "hybrid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Hudi table. Same vector index. Just added a SQL filter.\n",
    "\n",
    "---\n",
    "## 6. Analytics Dashboard\n",
    "\n",
    "Now we switch hats — we're an analyst on the job platform team. Which companies are hiring the most? What roles dominate the market? We query the **exact same table**. No ETL to a separate warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the jobs_table view loaded from the same Hudi table in cell 11.\n",
    "# Filter on job_id to exclude any Hudi metadata partition rows.\n",
    "\n",
    "company_df = spark.sql(\"\"\"\n",
    "    SELECT company, count(*) as job_count\n",
    "    FROM jobs_table\n",
    "    WHERE job_id LIKE 'job_%'\n",
    "    GROUP BY company\n",
    "    ORDER BY job_count DESC LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "title_df = spark.sql(\"\"\"\n",
    "    SELECT title, count(*) as title_count\n",
    "    FROM jobs_table\n",
    "    WHERE job_id LIKE 'job_%' AND title IS NOT NULL\n",
    "    GROUP BY title\n",
    "    ORDER BY title_count DESC LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].barh(company_df[\"company\"], company_df[\"job_count\"], color=\"green\")\n",
    "axes[0].set_title(\"Hiring Activity: Postings by Company\")\n",
    "axes[0].set_xlabel(\"Number of Postings\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(title_df[\"title\"], title_df[\"title_count\"], color=\"skyblue\")\n",
    "axes[1].set_title(\"Most Common Data Science Roles\")\n",
    "axes[1].set_xlabel(\"Number of Postings\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\u2713 Dashboard generated from the same Hudi table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**One table. Vector search, hybrid search, and analytics. No data copying, no separate vector database.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}